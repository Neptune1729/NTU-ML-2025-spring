{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neptune1729/NTU-ML-2025-spring/blob/main/mlhw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ],
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DWQh-lq8GuwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4efee214-c3fb-4300-b28e-5dd8fa4207b4"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "#%cd [change to the directory you prefer]\n",
        "%cd drive/MyDrive/NTU-ML-2025/hw1"
      ],
      "metadata": {
        "id": "P_5Tf1rMHBQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "56e4daa9-22cc-4ea3-bde4-78b494ead5d8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/NTU-ML-2025/hw1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5JywoPOO_Oll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec7c74ee-e4db-4027-ac39-8a0ec3f423d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp312-cp312-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m131.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (3.4.4)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (4.13.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (2.32.4)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from lxml_html_clean) (6.0.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.15.0)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.11.12)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->googlesearch-python) (3.11)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.23.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: websockets\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107332 sha256=58cd517aaf95963c8daeba08eeb1bc312fbb7c91751efb316c9844b4242a608e\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built websockets\n",
            "Installing collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.19.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.52.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "gradio-client 1.14.0 requires websockets<16.0,>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.3 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "kX6SizAt_Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e089f07-fb2e-4072-9878-2ed1158b3f83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ScyW45N__Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76f2a785-f0a7-47fd-801d-b40dba17c44b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0.1,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "8dmGCARd_Oln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf615249-402d-4078-cc74-a7ba084d7f07"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期以鄉郊風味而聞名，她在2006年的首張專輯《Taylor Swift》獲得了商業上的成績。隨後，she推出了多张专辑，其中包括 《Fearless》（2010）、_1989（）和 _Reputation （）。她的歌曲經常探討愛情、友誼以及個人生活的主题。\n",
            "\n",
            "泰勒絲在音樂界取得許多少項榮譽，她獲得了13座格萊美獎，成為史上最年輕的人類贏得該殊荣。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一名善於理解與重述問題的助理。\"\n",
        "        \"你的工作是從冗長、雜亂或包含許多無關背景資訊的描述中，\"\n",
        "        \"提煉出真正需要回答的核心問題。\"\n",
        "        \"使用中文時，只會使用繁體中文回答。\",\n",
        "    task_description=\"請閱讀使用者提供的原始問題描述，完成以下工作：\\n\"\n",
        "        \"1. 忽略和提問無關的情緒化語句、閒聊、垃圾話或過多背景細節。\\n\"\n",
        "        \"2. 找出使用者真正想解決的核心問題是什麼。\\n\"\n",
        "        \"3. 用 1～2 句清楚、完整的問句重寫這個核心問題。\\n\"\n",
        "        \"4. 不要回答問題，只需要輸出重寫後的核心問題。\\n\"\n",
        "        \"5. 請以「核心問題：XXX」的格式輸出，並全程使用繁體中文。\",\n",
        ")\n",
        "\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"你是一名專門為搜尋系統提取關鍵詞的專家。\"\n",
        "        \"你熟悉如何從自然語言問題中抽取適合檢索的關鍵詞與關鍵短語。\"\n",
        "        \"你會優先保留能幫助提升檢索精確度與召回率的資訊。\",\n",
        "    task_description=\"請根據給定的問題，完成以下工作：\\n\"\n",
        "        \"1. 從問題中提取 3～8 個關鍵詞或關鍵短語。\\n\"\n",
        "        \"2. 優先保留專有名詞、重要實體名稱、時間、地點、技術術語等。\\n\"\n",
        "        \"3. 如有必要，可以補充常見的英文寫法或縮寫，以提升搜尋效果。\\n\"\n",
        "        \"4. 不要解釋問題內容，也不要回答問題本身。\\n\"\n",
        "        \"5. 請僅輸出關鍵詞列表，使用半形逗號「,」分隔，不要加多餘說明文字。\\n\"\n",
        "        \"6. 關鍵詞本身可以是中文或英文，但說明文字一律使用繁體中文。\",\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
        "    task_description=\"請回答以下問題：\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    \"\"\"\n",
        "    整體 RAG pipeline：\n",
        "    1. 問題重寫（去掉無關內容，提取核心問題）\n",
        "    2. 關鍵詞抽取（為檢索服務）\n",
        "    3. 根據關鍵詞檢索文檔\n",
        "    4. 將 Context + Question 一起交給 QA agent 回答\n",
        "    \"\"\"\n",
        "\n",
        "    # ---------- 1. 用 question_extraction_agent 清洗問題 ----------\n",
        "    refined = question_extraction_agent.inference(question)\n",
        "\n",
        "    # 預期格式：「核心問題：XXX」\n",
        "    if isinstance(refined, str) and refined.startswith(\"核心問題\"):\n",
        "        # 嘗試去掉「核心問題：」這類前綴\n",
        "        core_question = refined.split(\"：\", 1)[-1].strip()\n",
        "    else:\n",
        "        core_question = refined.strip() if isinstance(refined, str) else question\n",
        "\n",
        "    # ---------- 2. 用 keyword_extraction_agent 抽關鍵詞 ----------\n",
        "    keywords_raw = keyword_extraction_agent.inference(core_question)\n",
        "    # 預期輸出類似：\"LLaMA-3.1-8B, RAG, 檢索增強生成\"\n",
        "    if isinstance(keywords_raw, str):\n",
        "        keyword_list = [k.strip() for k in keywords_raw.split(\",\") if k.strip()]\n",
        "    else:\n",
        "        keyword_list = []\n",
        "\n",
        "    # 如果關鍵詞抽不出來，就退化成用整個問題去檢索\n",
        "    if not keyword_list:\n",
        "        keyword_list = [core_question]\n",
        "\n",
        "    # ---------- 3. 根據關鍵詞做檢索（這裡要換成你作業給的函數） ----------\n",
        "    # ⚠️⚠️⚠️ 這裡非常重要：把下面這一行函數名，改成你 Notebook 裡的檢索函數 ⚠️⚠️⚠️\n",
        "    # 例如：retrieved_passages = search_documents(keyword_list, top_k=5)\n",
        "    # 或：  retrieved_passages = bm25_search(keyword_list, k=5)\n",
        "    # 我暫時先寫成一個假名：\n",
        "    retrieved_passages = []\n",
        "    try:\n",
        "        retrieved_passages = search_documents(keyword_list, top_k=5)\n",
        "        # 如果你們接口不同，例如 search_documents(keywords: str)，那就改掉這行\n",
        "    except NameError:\n",
        "        # 如果還沒實作或函數名不對，至少不讓程式崩掉\n",
        "        retrieved_passages = []\n",
        "\n",
        "    # ---------- 4. 構造 Context，控制長度 ----------\n",
        "    context_chunks = []\n",
        "    for i, passage in enumerate(retrieved_passages):\n",
        "        # passage 可能是 dict / (title, text) / 純字串，請按你們實際格式改\n",
        "        if isinstance(passage, str):\n",
        "            text = passage\n",
        "        elif isinstance(passage, dict) and \"text\" in passage:\n",
        "            text = passage[\"text\"]\n",
        "        else:\n",
        "            text = str(passage)\n",
        "        context_chunks.append(f\"[Passage {i+1}]\\n{text}\")\n",
        "\n",
        "    context = \"\\n\\n\".join(context_chunks)\n",
        "\n",
        "    # 粗暴做個長度截斷，防止超過上下文：\n",
        "    max_chars = 8000\n",
        "    if len(context) > max_chars:\n",
        "        context = context[:max_chars]\n",
        "\n",
        "    # ---------- 5. 把 Context + Question 統一交給 qa_agent ----------\n",
        "    qa_input = f\"\"\"\n",
        "以下是根據使用者問題所檢索到的相關資料（Context），以及整理後的問題（Question）。請據此回答問題。\n",
        "\n",
        "[Context]\n",
        "{context}\n",
        "\n",
        "[Question]\n",
        "{core_question}\n",
        "\"\"\"\n",
        "\n",
        "    answer = qa_agent.inference(qa_input)\n",
        "    return answer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer the questions using your pipeline!"
      ],
      "metadata": {
        "id": "P_kI_9EGB0S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ],
      "metadata": {
        "id": "PN17sSZ8DUg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"123456\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "plUDRTi_B39S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497fcc69-0d35-4764-dc2b-e920652d92ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 根據我的資料，「虎山雄風飛揚」是國立臺灣師範大學的校歌。\n",
            "2 根據我查到的資料，NCC（國家通訊傳播委員會）規定境外郵購自用產品的審核費為新台幣 1,000 元。\n",
            "3 根據資料，第一代 iPhone 是由史蒂夫·喬布斯（Steve Jobs）發表。\n",
            "4 根據台灣大學的進階英文免修申請規定，托福網路測驗 TOEFL iBT 的最低分數要求是 80 分。\n",
            "5 觸地 try（也稱為腳踩試）在 Rugby Union 中是一種特殊的得分方式。根據規則，當球員成功將對手隊伍的一名選手推入自己的終點區，並且自己或同伴同時著陸於該處時，就會被判定成觸地 try。  一旦發生這樣的情況，那麼就可以獲得五個分（5 points）。\n",
            "6 根據我的資料，卑南族的祖先發源地ruvuwa'an位於台東縣。\n",
            "7 很抱歉，我無法找到相關的資料。\n",
            "8 根據歷史資料，電磁感應定律是由詹姆斯·克拉ーク・マ克斯韋爾在19世紀中期發現的。\n",
            "9 根據資料，距離國立臺灣史前文化博物館最近的臺鐵車站是台中火车总站在。\n",
            "10 根據給定的數字，20 加上 30 的結果是：  40\n",
            "11 很抱歉，但我無法提供關於未來的資訊（例如NBA 2025年的交易）。\n",
            "12 很抱歉，我無法提供最新的資訊或未來事件。根據我目前可用的資料，2024年美國總統大選尚在進行中，並沒有確定的勝出者。如果您需要關於過去的大选結果或者其他相關信息，可以試著問我的！\n",
            "13 根據我的知識，Llama-3.1 系列模型中，我是 LLaMA 8B 的一部分。雖然我沒有直接的資訊關於 Meta 所有版本中的參數量，但基礎上來說較小型別通常會比大尺寸更少。  根據我的知識，Llama-3.1 系列模型中，我是 LLaMA 8B 的一部分。\n",
            "14 根據國立臺灣大學學則，停修課程的限制是三門。\n",
            "15 根據我的資料，DeepSeek公司的母 公司是 Ocean Infinity。\n",
            "16 很抱歉，我無法提供最新的資訊，因為我的知識截止日期是2023年12月。然而根據我所知道的情況，目前尚未結束賽季，因此還不能確定 2024 年 NBA 總冠軍隊伍。但如果你想問的是前幾年的總決赛，我可以提供相關資訊。  例如：  *   前一年（23-24年）的NBA总决战是金州勇士队与波特兰开拓者之间的比赛。 若您有其他問題或需要更多信息，請告訴我。\n",
            "17 根據化學知識，碳原子與其他同類元素形成三鍵的結構稱為烯（Alkene）。在這種情況下，一個或多个雙键連接了兩組相同或者不同的有機基團。\n",
            "18 根據資料，阿倫·圖靈（Alan Turing）被譽為「計算機科學之父」。他是一位英國數理逼近家和電腦科学家的先驅，他對於現代電子计算机的發展做出了重要貢獻。\n",
            "19 根據資料，臺灣玄天上帝信仰的進香中心位於新北市貢寮區。\n",
            "20 根據資料，Windows 作業系統是微軟公司（Microsoft）所開發的產品。\n",
            "21 根據資料顯示，官將首是臺灣所創立的陣頭之一，它主要負責擔當主神駕前侍衛。這意味著在各種宗教儀式或慶典活動中，這些人士會站在領導者（通常為天公）之前，以保護和陪伴他們，同時也代表了社會秩序的象徵。  官將首陣頭成立於清治時期，是臺灣本地發展出的民間信仰組織。它們在各種宗教活動中扮演著重要角色，並且受到當地方族人的尊敬和支持。在這些人士身上，既有軍事的防衛意義，也具有社會秩序維護的一面。  因此，可以說官將首陣頭是臺灣民間信仰中的獨特象徵之一，它不僅代表著領導者的保護，更體現了當地人的文化和傳統。\n",
            "22 《咒》是一部奇幻小說，內容涉及到多種神祇和邪惡力量。根據我的知識庫，我們可以知道，在這本書中，有一個名為 \"阿卡曼\"（Aramand Danton）或是簡稱的「D」的人物，他被描述成一位強大的黑暗法師，擁有操控咒力的能力。  然而，如果你問的是《惡靈古堡》系列中的邪神，那麼答案就不同了。根據我的知識庫，我們可以知道，在這個遊戲中，有一個名為 \"阿比斯\"（Abaddon）或是簡稱的「D」的人物，他被描述成一位強大的黑暗法師，擁有操控惡靈和咒力的能力。  如果你能提供更多資訊或者更具體地指出《哪本書》中的邪神，我們就可以給予更加準確且詳細的答案。\n",
            "23 根據我的資料，我無法找到任何一首歌曲的標題是「短暫交會就此分岔」。如果你能提供更多資訊或上下文，可能我可以幂助您找出答案。\n",
            "24 很抱歉，我無法在本平台找到相關資料。\n",
            "25 根據最新的資訊，輝達（NVIDIA）的GeForce RTX系列目前已經到了第五代。2023年8月發布了RTS 40和Ada Lovelace架構下的 GeForce GTX 和 Quadro卡，但並未公佈新的「50」級別顯示核心。  所以，最最新的輝達出現過的是GeForce RTX系列第四世（Lovelance）\n",
            "26 根據我的資料，沒有相關的資訊顯示大S是在哪個國家旅遊時去世。\n",
            "27 根據歷史記錄，萬有引力定律是由英國物理學家艾薩克·牛頓（Isaac Newton）於1687年發表的。他在他的著作《哲理自然科論》（Philosophiæ Naturalis Principia Mathematica）的第三卷中提出這個概念。\n",
            "28 根據資料，台鵠開示計畫「TAIHUCAIS」的英文全名是Taiwan Indigenous Cultural Assets Information System。\n",
            "29 答案是《終極殺手》。這句經典台詞由阿諾·施瓦辛格飾演的角色達德里克‧傑恩（Dutch）說出，成為了1980年代後期至1994年間的一個流行文化符號，並被多次引用和模仿於其他電影、電視節目及廣告中。\n",
            "30 水的化學式是H2O。\n",
            "31 很抱歉，我無法提供李宏毅《機器學習》2023年春季班的內容，因為我沒有存取到這些資料。\n",
            "32 根據資料，臺灣公立獨립學院的唯一一間是國防管理学院（National Defense University）。\n",
            "33 BitTorrent 協議使用了一種稱為「分片」(Piece) 的機制來確保新加入的節點能夠從其他种子隨机獲得部分資料。這個過程如下：  1. **建立 Tracker**:當一個新的 torrent 文件被創建時，會與一台Tracker伺服器連接起，這臺tracker負責管理所有相關到該torrent文件的人員。 2..**分片**(Piece) 的產生: 當資料檔案傳輸開始後, 便將整個大型的 torrent 文件切割成許多小塊稱為「piece」(每一份大小約在10-100KB之間), 這些 piece 之中包含了torrent文件內容的一部分。 3. **節點加入**: 當一個新节点想要下载该Torrent时，它会向Tracker发送一个请求，要求获取 torrent 文件的信息。 Tracker 会返回给它一些种子列表（Seeders）和其他活跃用户连接点。  4..**DHT**(分布式哈希表) : Bit Torrent 協議使用 Distrubuted Hash Table (簡稱為「dht」), 這是一種分佈在各個節點上的資料庫, 使得每一個node都能夠存取其他 node 的資訊。  5. **Peer Discovery**: 当新节点加入 torrent 时，它会向 Tracker 或 DHT 发送一个请求，要求获取活跃用户列表。Tracker 和Dht 会返回给它一些种子和连接点的信息。 6..**Piece Requesting:** 新節點會根據tracker或dhat回傳給他的資料, 來發出對其他node的一個request請求某一份piece。  7. **Peer Connection**: 当新节点收到一个 piece 的请求时，它会与该 node 建立连接，开始传输数据。这个过程中使用了 TCP 协议来确保可靠的文件下载。 8..**Piece Downloading:** 新節點會從其他node接著下載資料, 這個過程是由TCP協議來控制。  9. **Verify Piece**: 下载完成后，新节点会验证该 piece 是否正确无误。如果有问题，它可以要求重新传输或拒绝接受错误的数据。 10..**Piece Assembly:** 一旦所有piece都下载并且驗證無誤\n",
            "34 沒有提供任何相關的資料或問題描述。若您能夠分享更多資訊，我將會盡力幫助你回答你的疑問！\n",
            "35 很抱歉，但我無法在網路上找到任何關於戈芬氏鳳頭鸚鵡喜歡的乳酪口味相關資訊。\n",
            "36 根據我的知識，Xpark水族館的企鵝寶貝最後被命名為「阿布」。\n",
            "37 根據國立臺灣大學的資訊，物理治療學系通常需要修業四年。\n",
            "38 根據《BanG Dream!》的資料，角色Rinko Shirokane有「呼嘿」、「呵嗦」的笑聲習慣。\n",
            "39 根據資料，日本戰國時代被稱為「甲斐之虎」的人物是武田信玄。\n",
            "40 根據提供的資料，我們無法得知王肥貓同學最有可能修哪一門課。然而，基於一般情況和大學生的常見選擇，可以推測一些可能性：  1. **計算機概論**：許多新生都會在第一年或第二年的時候上這堂课。 2. 信息安全**:王肥貓同學可能對資訊科技感興趣，修習相關課程如網路攻防、資料保護等。\n",
            "41 根據我的資料，2024年的第42回《極限體能王SASUKE》首播的日期是10月7日。\n",
            "42 根據歷史資料，出身於利嘉部落後來成為初鹿頭目的漢人是林爽文。\n",
            "43 根據我的資料，BanG Dream! Ave Mujica的片頭曲是《》。\n",
            "44 Linux作業系統最早於1991年首次發布。由林納斯·托瓦茲（Linus Torvalds）在芬蘭赫爾辛基大學開源，第一個版本是0.01版，也就是 Linux 伺服器的前身。  根據史料記載，在9月17日時，他將Linux作業系統發布給網路社群，並且開始與其他程式設計師合作。這標誌著linux操作系统正式出現了，之後便迅速發展成現在我們所熟悉的一個開源的、自由又強大的operating system。  因此，可以說1991年是Linux作業系統最早發布年的里見點！\n",
            "45 根據我的資料，卑南族的 Likavung 部落中文名稱是「利卡邦部」。\n",
            "46 根據你的問題，我們知道紅茶是一種常見的飲料。然而，關於它是發酵還是不发煮（或稱為生產）的，這個答案有點複雜。  一般來說，大部分的人認爲黑tea和綠 tea都是不被釋放，而白teas則可能會有一些微量的氧化。然而，紅茶是通過發酵過程製造出特殊風味與口感，這個特徽使得它們在市場上有很大的競爭力。  所以，我可以回答你：一般來說，大部分的人認爲黑tea和綠 tea都是不被釋放，而白teas則可能會有一些微量的氧化。然而，紅茶是通過發酵過程製造出特殊風味與口感，所以它們在市場上有很大的競爭力。  但如果你問的是什麼樣的情況下，我可以回答：當然，如果我們說到黑tea和綠 tea，它的確不被釋放。\n",
            "47 根據《遊戲王》的資料，當你以「真紅眼黑龍」與 「 黑魔導 」作為融合素材時，你會得到 \"Red-Eyes B. Dragon\" 的強化版本——  《Black Luster Soldier - Envoy of the Evening Twilight》\n",
            "48 根據資料顯示，豐田萌繪在《BanG Dream!》中聲演角色是 Rosia。\n",
            "49 根據 Rugby Union 的規則，9 號球員的正式名稱是「掃劫半（Scrum-half）」。\n",
            "50 根據太陽系的最新定義，曾被視為行星但後來降格成矮 行 星的是冥王-star（Pluto）。\n",
            "51 根據資料，臺灣最早成立的野生動物救傷單位位於台北市內。\n",
            "52 根據我的資料，特生中心在2023年改名後的名字是「台北市立兒童新樂園」。\n",
            "53 根據您的問題，我們可以知道您是問一篇論文中提到的模型名稱。雖然我不知道你要查的那個文章，但是如果有相關資訊，那麼這裡有一些可能與題目相符的一般知識：  1.  Instruction-Following Speech Language Model WithoutSpeechInstruction-Tuning Data： 這是一種新的語言模式，主要是針對無指導性的口音訓練模型。 2.BERT（Bidirectional Encoder Representations from Transformers）： BERN 是一款基於轉換器的預測機制，可以用來處理自然语言处理中的許多問題，如文本分类、情感分析等。\n",
            "54 根據太陽系的資料，體積最大的行星是土壤。\n",
            "55 根據你的問題，我們可以知道你在問的是哪一族的原住民族語言與其他南島系臺灣十六個法定認定的原始民間口頭傳承之母親話最不相近。  答案是：布農人。\n",
            "56 沒有提供相關的資料或問題內容，我們無法確定你在問什麼。能夠給予更多資訊嗎？\n",
            "57 根據資料，「embiyax namu kana」是阿美族的打招呼用語。\n",
            "58 根據我的知識，這句話「鄒與布農，永久美麗」是來自於台灣原住民的典故。這兩個部落分別為台南市東西向走廊上的泰雅族（也稱作達悟）和高雄縣北方山區中的阿猴語支下的卑詩人等布農人的同源詞，意思是「鄒與 布努」、「永久美麗」。\n",
            "59 很抱歉，我們無法找到相關的資料。能夠提供更多資訊或背景嗎？\n",
            "60 根據卑南族的傳說，姊弟Tuku及Sihasihau分別創建了阿美人和布農人的部落。\n",
            "61 根據《終極一班》的劇情，「KO榜」的第一名是何東昇。\n",
            "62 根據 Linux kernel 的 CFS (Completely Fair Scheduler) 流程調度器的設計，C FS 使用紅黑樹（Red-Black Tree）來儲存排行相關資訊。這種資料結構能夠有效地實現快速查找、插入和刪除流覽節點，並且保證了對所有進程序的一致性。  在 CFS 中，使用紅黑樹的主要目的是為每個正在執行動作（Running）的程式建立一個叫做 \"runqueue\" 的結構體。這些 runqueues 會根據各自所屬 CPU 處理器進行組織，並且通過對應於該處 理 器上的 red-black tree 結合在一起。  紅黑樹能夠有效地實現快速查找、插入和刪除流覽節點，從而使得 CFS 能够更好 地管理系統中的各個進程。\n",
            "63 根據歷史資料，諾曼第登陸的作戰代號是「奧運會」（Operation Overlord）。\n",
            "64 根據《Cytus II》遊戲的資料，「Body Talk」是由角色 Tyni 演唱。\n",
            "65 根據我的知識，李琳山教授的演講被稱為「一場沒有結尾的小說」。\n",
            "66 根據最新的資訊，NVIDIA RTX 5090 顯卡並沒有被正式公布或發表。因此，我們無法確定它所具備哪些特性。  但如果你是指 NVIDIA 的未來產品線，那麼我可以提供一些相關信息：  *   根據市場傳言和猜測，NVIDIA RTX 50 系列可能會使用 GDDR7 記憶體技術。          *根基上，我們無法確定RTx5090的VRAM大小。\n",
            "67 根據我的資料，2024年世界棒球12強賽的冠軍隊伍尚未公布。因為我沒有收到最新消息或更新資訊，所以無法提供確切答案。如果你需要更正或者進一步詳情，我可以幫助查找相關信息給您！\n",
            "68 根據文學史的記載，中國四大奇書是指《西遊记》、《水浒传》，以及兩部唐詩集：_金瓶梅和三國演義\n",
            "69 根據中華民國的時間系統，子時在24小 時制中的對應範圍是23點到1点。\n",
            "70 根據你的問題，避免錯過時限來完成任務的排程演算法稱為「實時間系統」（Real-Time System）中的 \"硬性\" 時間限制下的 **Deadline Scheduling** 或是簡單地叫做 \"**即期式預約**(Hard Real-time)**\".  但在一般作業系统中，避免錯過時限來完成任務的排程演算法稱為「實時間系統」（Real-Time System）中的 \"硬性\" 時間限制下的 **Deadline Scheduling** 或是簡單地叫做 \"**即期式預約**(Hard Real-time)**\".  但在一般作業系统中，避免錯過時限來完成任務的排程演算法稱為「先到者優惠」（First-Come- First-Served, FCFS）或 **最短工作首位排序**（Shortest Job Next,SJN）。\n",
            "71 根據《刀劍神域》系列的資料，「C8763」代號對應於黑乃（Kyoji Shiguragawa）持有的特定技能。\n",
            "72 根據《斯卡羅》劇的設定，柴城位於現今中國四川省德陽市境內。\n",
            "73 根據Google Colab的資訊，Colaboratory Pro+版包含A100高級GPU和TPU（Tensor Processing Unit）的使用權限，而不是標準版本。因此，我們可以得出結論：  答案： Google  的「Pro +」 版本\n",
            "74 根據我的知識，李宏毅老師是國立臺灣大學資訊工程學系的教授。他開設了許多機器 學習相關課程。因此，我們可以確定他的教室屬於電腦科學院或信息傳播科技教育中心\n",
            "75 根據一般大學的規定，大三學生通常需要修滿一定分數才能不用簽減免申請書。這個標準可能會有所不同，取決於學校或系級。  在大部分的情況下，一般來說要達到以下幾種情形：  1. **全日制**：一般需完成學期修滿 18 分數以上（含）。 2..非教職人員子女、軍人及退伍兵眷生等特殊優惠政策的考慮，可能會有所不同。  然而，這些標準並不一致，我們建議你直接查詢你的大學或系級相關規定，以確保正確認知。\n",
            "76 根據我的資料，Neuro-sama 的 Live2D 模型最初是使用 VTube Studio 預設角色。\n",
            "77 根據我的資料，「從零開始的異世界生活 第三季」動畫中，被劫持的人是愛蜜莉雅（Emilia），而她的綁架者則為尤格尼斯·克拉德爾諾夫。\n",
            "78 根據《海綿寶宝》第五季的劇情，刺破泡沫紅眼幇被擊敗的是布魯克林。\n",
            "79 根據植物學的分類，玉米屬於單子葉草本。\n",
            "80 根據中華民國陸軍的官方資料，該部隊最著名且被廣泛認可為其象徵性的歌曲是《八百壯士》。但在2019年起，這首原本由黃自強作詞、許景淳填調所改編而成，並以「中華民國陸軍官兵之友會」名義出版的版本被廢止，取代的是一支新歌曲《八百壯士》原版。\n",
            "81 根據台大電資學院的規定，計算機科學與信息工程系（CSIE）的物理、化成和生物課程要求相對較低。\n",
            "82 根據月球地形的分布，以下五個區域位於不面對地球（背側）的位置：  1. 南極-亞平寧高原：是最大的單一陸塊，也是一座古老的大型火山。 2.Mare Fecunditatis (豐饒海) 的西北部 3.Lacus Somniorum(夢湖) 4.Oceanus Procellarurn（風暴洋） 5. Mare Australe （南方之月）\n",
            "83 根據資料顯示，莫扎特的《C♯小調第14號鋼琴奏鳴曲》又稱為「費加洛婚禮」或是K. 491。\n",
            "84 阿米斯音樂節（Amnesia Festival）是由法國歌手M83所舉辦的。\n",
            "85 根據 Poppy Playtime - Chapter 4 的資料，黏土人的名字是 Huggy Wugglepants。\n",
            "86 根據我的資料，賓茂村位於臺灣屏東縣三地門鄉。\n",
            "87 根據資料，米開朗基羅的《大衛》雕像最初是在佛罗伦萨創作。\n",
            "88 根據我的資料，除了蔣中正之外，被晉升為特級上將的另一位軍官是何應欽。\n",
            "89 根據資料顯示，2012年第二賽季英雄聯盟世界大赛的冠军是韓國戰隊SK Telecom T1。\n",
            "90 根據日本麻將規則，非莊家一開始的手牌張數是14隻。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "execution_count": 12,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}