{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Neptune1729/NTU-ML-2025-spring/blob/main/mlhw1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TFwaJir_Olj"
      },
      "source": [
        "# ML2025 Homework 1 - Retrieval Augmented Generation with Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tQHdH2k_Olk"
      },
      "source": [
        "## Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we will mount your own Google Drive and change the working directory."
      ],
      "metadata": {
        "id": "-_ZkNxqGGhdl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DWQh-lq8GuwZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c031c3-fad8-427c-93e4-d9b126d4af2a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the working directory to somewhere in your Google Drive.\n",
        "# You could check the path by right clicking on the folder.\n",
        "#%cd [change to the directory you prefer]\n",
        "%cd drive/MyDrive/NTU-ML-2025/"
      ],
      "metadata": {
        "id": "P_5Tf1rMHBQ-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d452c187-ab9c-4a43-eb60-d29475499671"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meta-Llama-3.1-8B-Instruct-Q8_0.gguf  private.txt  public.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGx000oZ_Oll"
      },
      "source": [
        "In this section, we install the necessary python packages and download model weights of the quantized version of LLaMA 3.1 8B. Also, download the dataset. Note that the model weight is around 8GB. If you are using your Google Drive as the working directory, make sure you have enough space for the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5JywoPOO_Oll",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2eb5ef9d-1eb5-4efa-ee0f-b7d7502aa1a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
            "Collecting llama-cpp-python==0.3.4\n",
            "  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.4-cu122/llama_cpp_python-0.3.4-cp312-cp312-linux_x86_64.whl (445.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.2/445.2 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (4.15.0)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (2.0.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.3.4)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.12/dist-packages (from llama-cpp-python==0.3.4) (3.1.6)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2>=2.11.3->llama-cpp-python==0.3.4) (3.0.3)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.4\n",
            "Collecting googlesearch-python\n",
            "  Downloading googlesearch_python-1.3.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (3.4.4)\n",
            "Collecting requests-html\n",
            "  Downloading requests_html-0.10.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting lxml_html_clean\n",
            "  Downloading lxml_html_clean-0.4.3-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: beautifulsoup4>=4.9 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (4.13.5)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.12/dist-packages (from googlesearch-python) (2.32.4)\n",
            "Collecting pyquery (from requests-html)\n",
            "  Downloading pyquery-2.0.1-py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting fake-useragent (from requests-html)\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting parse (from requests-html)\n",
            "  Downloading parse-1.20.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting w3lib (from requests-html)\n",
            "  Downloading w3lib-2.3.1-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting pyppeteer>=0.0.14 (from requests-html)\n",
            "  Downloading pyppeteer-2.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from lxml_html_clean) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.9->googlesearch-python) (4.15.0)\n",
            "Collecting appdirs<2.0.0,>=1.4.3 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: certifi>=2023 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (2025.11.12)\n",
            "Requirement already satisfied: importlib-metadata>=1.4 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (8.7.0)\n",
            "Collecting pyee<12.0.0,>=11.0.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading pyee-11.1.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from pyppeteer>=0.0.14->requests-html) (4.67.1)\n",
            "Collecting urllib3<2.0.0,>=1.25.8 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading urllib3-1.26.20-py2.py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.1/50.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<11.0,>=10.0 (from pyppeteer>=0.0.14->requests-html)\n",
            "  Downloading websockets-10.4.tar.gz (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.20->googlesearch-python) (3.11)\n",
            "Collecting cssselect>=1.2.0 (from pyquery->requests-html)\n",
            "  Downloading cssselect-1.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=1.4->pyppeteer>=0.0.14->requests-html) (3.23.0)\n",
            "Downloading googlesearch_python-1.3.0-py3-none-any.whl (5.6 kB)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Downloading requests_html-0.10.0-py3-none-any.whl (13 kB)\n",
            "Downloading lxml_html_clean-0.4.3-py3-none-any.whl (14 kB)\n",
            "Downloading pyppeteer-2.0.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.9/82.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading parse-1.20.2-py2.py3-none-any.whl (20 kB)\n",
            "Downloading pyquery-2.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading w3lib-2.3.1-py3-none-any.whl (21 kB)\n",
            "Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading cssselect-1.3.0-py3-none-any.whl (18 kB)\n",
            "Downloading pyee-11.1.1-py3-none-any.whl (15 kB)\n",
            "Downloading urllib3-1.26.20-py2.py3-none-any.whl (144 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.2/144.2 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: websockets\n",
            "  Building wheel for websockets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for websockets: filename=websockets-10.4-cp312-cp312-linux_x86_64.whl size=107329 sha256=79d2e0241fb816e76d0855a4f3a9292256651399681e117faeeadbcaffb3ce15\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/cf/6d/5d7e4c920cb41925a178b2d2621889c520d648bab487b1d7fd\n",
            "Successfully built websockets\n",
            "Installing collected packages: parse, appdirs, websockets, w3lib, urllib3, pyee, lxml_html_clean, fake-useragent, cssselect, pyquery, pyppeteer, bs4, requests-html, googlesearch-python\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.5.0\n",
            "    Uninstalling urllib3-2.5.0:\n",
            "      Successfully uninstalled urllib3-2.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-adk 1.17.0 requires websockets<16.0.0,>=15.0.1, but you have websockets 10.4 which is incompatible.\n",
            "google-genai 1.51.0 requires websockets<15.1.0,>=13.0.0, but you have websockets 10.4 which is incompatible.\n",
            "dataproc-spark-connect 0.8.3 requires websockets>=14.0, but you have websockets 10.4 which is incompatible.\n",
            "yfinance 0.2.66 requires websockets>=13.0, but you have websockets 10.4 which is incompatible.\n",
            "gradio-client 1.13.3 requires websockets<16.0,>=13.0, but you have websockets 10.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed appdirs-1.4.4 bs4-0.0.2 cssselect-1.3.0 fake-useragent-2.2.0 googlesearch-python-1.3.0 lxml_html_clean-0.4.3 parse-1.20.2 pyee-11.1.1 pyppeteer-2.0.0 pyquery-2.0.1 requests-html-0.10.0 urllib3-1.26.20 w3lib-2.3.1 websockets-10.4\n",
            "--2025-11-24 13:17:49--  https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
            "Resolving huggingface.co (huggingface.co)... 3.165.160.59, 3.165.160.61, 3.165.160.11, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.165.160.59|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251124T131750Z&X-Amz-Expires=3600&X-Amz-Signature=086273118caac52c52aa991518ff201c36bbbc7520429178ea66cec8a1715907&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1763993870&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Mzk5Mzg3MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=mx86Un4165n6sui9c0uFLsUPHvnpreZ%7Ec-9kYYR1%7EARaEPpuzYZljOb%7E0jKsGrncYaZjO2lrwzIXGYeRADXpu1qoLzB2GmAoBijo722NEH87eVquMnwMDjsbrfg0CpCQZYYdsBCOjGBEGYFrGXekReaGxZiL%7EPNMxqBk1QKl7X34fitHTpHLZs3r44Qhq1QCpQvIh3mwyR-RPqvehn3HNYn9W7l9bjl1k%7EWVlsTFv7i2E8MOh-Lk8vR2FU0fZc09%7E0G2hzWqVWaZSmGNt2Qi34CAxa6Hrmskz0nbo5b2VPskfp93HurpvBtwqoNq%7Exlqi7f0qpIQQb3M6ognZ6EkAQ__&Key-Pair-Id=K2L8F4GPSG1IFC [following]\n",
            "--2025-11-24 13:17:50--  https://cas-bridge.xethub.hf.co/xet-bridge-us/669fce02988201fd4f9ceddc/13ba7de6d825796cd4846a9428031ca1be96a4f615bce26c19aafb27a9cf8a2c?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Content-Sha256=UNSIGNED-PAYLOAD&X-Amz-Credential=cas%2F20251124%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20251124T131750Z&X-Amz-Expires=3600&X-Amz-Signature=086273118caac52c52aa991518ff201c36bbbc7520429178ea66cec8a1715907&X-Amz-SignedHeaders=host&X-Xet-Cas-Uid=public&response-content-disposition=inline%3B+filename*%3DUTF-8%27%27Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%3B+filename%3D%22Meta-Llama-3.1-8B-Instruct-Q8_0.gguf%22%3B&x-id=GetObject&Expires=1763993870&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc2Mzk5Mzg3MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2FzLWJyaWRnZS54ZXRodWIuaGYuY28veGV0LWJyaWRnZS11cy82NjlmY2UwMjk4ODIwMWZkNGY5Y2VkZGMvMTNiYTdkZTZkODI1Nzk2Y2Q0ODQ2YTk0MjgwMzFjYTFiZTk2YTRmNjE1YmNlMjZjMTlhYWZiMjdhOWNmOGEyYyoifV19&Signature=mx86Un4165n6sui9c0uFLsUPHvnpreZ%7Ec-9kYYR1%7EARaEPpuzYZljOb%7E0jKsGrncYaZjO2lrwzIXGYeRADXpu1qoLzB2GmAoBijo722NEH87eVquMnwMDjsbrfg0CpCQZYYdsBCOjGBEGYFrGXekReaGxZiL%7EPNMxqBk1QKl7X34fitHTpHLZs3r44Qhq1QCpQvIh3mwyR-RPqvehn3HNYn9W7l9bjl1k%7EWVlsTFv7i2E8MOh-Lk8vR2FU0fZc09%7E0G2hzWqVWaZSmGNt2Qi34CAxa6Hrmskz0nbo5b2VPskfp93HurpvBtwqoNq%7Exlqi7f0qpIQQb3M6ognZ6EkAQ__&Key-Pair-Id=K2L8F4GPSG1IFC\n",
            "Resolving cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)... 18.238.217.88, 18.238.217.126, 18.238.217.63, ...\n",
            "Connecting to cas-bridge.xethub.hf.co (cas-bridge.xethub.hf.co)|18.238.217.88|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8540775840 (8.0G)\n",
            "Saving to: ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’\n",
            "\n",
            "Meta-Llama-3.1-8B-I 100%[===================>]   7.95G  61.5MB/s    in 2m 9s   \n",
            "\n",
            "2025-11-24 13:19:59 (62.9 MB/s) - ‘Meta-Llama-3.1-8B-Instruct-Q8_0.gguf’ saved [8540775840/8540775840]\n",
            "\n",
            "--2025-11-24 13:19:59--  https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 4399 (4.3K) [text/plain]\n",
            "Saving to: ‘public.txt’\n",
            "\n",
            "public.txt          100%[===================>]   4.30K  --.-KB/s    in 0s      \n",
            "\n",
            "2025-11-24 13:20:00 (797 MB/s) - ‘public.txt’ saved [4399/4399]\n",
            "\n",
            "--2025-11-24 13:20:00--  https://www.csie.ntu.edu.tw/~ulin/private.txt\n",
            "Resolving www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)... 140.112.30.26\n",
            "Connecting to www.csie.ntu.edu.tw (www.csie.ntu.edu.tw)|140.112.30.26|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 15229 (15K) [text/plain]\n",
            "Saving to: ‘private.txt’\n",
            "\n",
            "private.txt         100%[===================>]  14.87K  --.-KB/s    in 0.001s  \n",
            "\n",
            "2025-11-24 13:20:01 (11.9 MB/s) - ‘private.txt’ saved [15229/15229]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python3 -m pip install --no-cache-dir llama-cpp-python==0.3.4 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122\n",
        "!python3 -m pip install googlesearch-python bs4 charset-normalizer requests-html lxml_html_clean\n",
        "\n",
        "from pathlib import Path\n",
        "if not Path('./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf').exists():\n",
        "    !wget https://huggingface.co/bartowski/Meta-Llama-3.1-8B-Instruct-GGUF/resolve/main/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\n",
        "if not Path('./public.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/public.txt\n",
        "if not Path('./private.txt').exists():\n",
        "    !wget https://www.csie.ntu.edu.tw/~ulin/private.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kX6SizAt_Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cbd615b-3539-4e75-ccd2-accc2a9b54e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "You are good to go!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception('You are not using the GPU runtime. Change it first or you will suffer from the super slow inference speed!')\n",
        "else:\n",
        "    print('You are good to go!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3iyc1qC_Olm"
      },
      "source": [
        "## Prepare the LLM and LLM utility function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T59vxAo2_Olm"
      },
      "source": [
        "By default, we will use the quantized version of LLaMA 3.1 8B. you can get full marks on this homework by using the provided LLM and LLM utility function. You can also try out different LLM models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtepTeT3_Olm"
      },
      "source": [
        "In the following code block, we will load the downloaded LLM model weights onto the GPU first.\n",
        "Then, we implemented the generate_response() function so that you can get the generated response from the LLM model more easily."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eVil2Vhe_Olm"
      },
      "source": [
        "You can ignore \"llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\" warning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "ScyW45N__Olm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "05434272-79df-441b-93e3-1bd18f0619d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_new_context_with_model: n_ctx_per_seq (16384) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "# Load the model onto GPU\n",
        "llama3 = Llama(\n",
        "    \"./Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\",\n",
        "    verbose=False,\n",
        "    n_gpu_layers=-1,\n",
        "    n_ctx=16384,    # This argument is how many tokens the model can take. The longer the better, but it will consume more memory. 16384 is a proper value for a GPU with 16GB VRAM.\n",
        ")\n",
        "\n",
        "def generate_response(_model: Llama, _messages: str) -> str:\n",
        "    '''\n",
        "    This function will inference the model with given messages.\n",
        "    '''\n",
        "    _output = _model.create_chat_completion(\n",
        "        _messages,\n",
        "        stop=[\"<|eot_id|>\", \"<|end_of_text|>\"],\n",
        "        max_tokens=512,    # This argument is how many tokens the model can generate, you can change it and observe the differences.\n",
        "        temperature=0.1,      # This argument is the randomness of the model. 0 means no randomness. You will get the same result with the same input every time. You can try to set it to different values.\n",
        "        repeat_penalty=2.0,\n",
        "    )[\"choices\"][0][\"message\"][\"content\"]\n",
        "    return _output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnHLwq-4_Olm"
      },
      "source": [
        "## Search Tool"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYM-2ZsE_Olm"
      },
      "source": [
        "The TA has implemented a search tool for you to search certain keywords using Google Search. You can use this tool to search for the relevant **web pages** for the given question. The search tool can be integrated in the following sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "bEIRmZl7_Oln"
      },
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from googlesearch import search as _search\n",
        "from bs4 import BeautifulSoup\n",
        "from charset_normalizer import detect\n",
        "import asyncio\n",
        "from requests_html import AsyncHTMLSession\n",
        "import urllib3\n",
        "urllib3.disable_warnings()\n",
        "\n",
        "async def worker(s:AsyncHTMLSession, url:str):\n",
        "    try:\n",
        "        header_response = await asyncio.wait_for(s.head(url, verify=False), timeout=10)\n",
        "        if 'text/html' not in header_response.headers.get('Content-Type', ''):\n",
        "            return None\n",
        "        r = await asyncio.wait_for(s.get(url, verify=False), timeout=10)\n",
        "        return r.text\n",
        "    except:\n",
        "        return None\n",
        "\n",
        "async def get_htmls(urls):\n",
        "    session = AsyncHTMLSession()\n",
        "    tasks = (worker(session, url) for url in urls)\n",
        "    return await asyncio.gather(*tasks)\n",
        "\n",
        "async def search(keyword: str, n_results: int=3) -> List[str]:\n",
        "    '''\n",
        "    This function will search the keyword and return the text content in the first n_results web pages.\n",
        "\n",
        "    Warning: You may suffer from HTTP 429 errors if you search too many times in a period of time. This is unavoidable and you should take your own risk if you want to try search more results at once.\n",
        "    The rate limit is not explicitly announced by Google, hence there's not much we can do except for changing the IP or wait until Google unban you (we don't know how long the penalty will last either).\n",
        "    '''\n",
        "    keyword = keyword[:100]\n",
        "    # First, search the keyword and get the results. Also, get 2 times more results in case some of them are invalid.\n",
        "    results = list(_search(keyword, n_results * 2, lang=\"zh\", unique=True))\n",
        "    # Then, get the HTML from the results. Also, the helper function will filter out the non-HTML urls.\n",
        "    results = await get_htmls(results)\n",
        "    # Filter out the None values.\n",
        "    results = [x for x in results if x is not None]\n",
        "    # Parse the HTML.\n",
        "    results = [BeautifulSoup(x, 'html.parser') for x in results]\n",
        "    # Get the text from the HTML and remove the spaces. Also, filter out the non-utf-8 encoding.\n",
        "    results = [''.join(x.get_text().split()) for x in results if detect(x.encode()).get('encoding') == 'utf-8']\n",
        "    # Return the first n results.\n",
        "    return results[:n_results]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC3zQjjj_Oln"
      },
      "source": [
        "## Test the LLM inference pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "8dmGCARd_Oln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db82be5a-2007-4ffc-c919-d5725ab03882"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "泰勒絲（Taylor Swift）是一位美國歌手、詞曲作家和音樂製作人。她出生於1989年，來自田納西州。她的音乐风格从乡村摇滚发展到流行搖擺，並且她被誉为当代最成功的女艺人的之一。\n",
            "\n",
            "泰勒絲早期以鄉郊風味而聞名，她在2006年的首張專輯《Taylor Swift》獲得了商業上的成績。隨後，she推出了多张专辑，其中包括 《Fearless》（2010）、_1989（）和 _Reputation （）。她的歌曲經常探討愛情、友誼以及個人生活的主题。\n",
            "\n",
            "泰勒絲在音樂界取得許多少項榮譽，她獲得了13座格萊美獎，成為史上最年輕的人類贏得該殊荣。\n"
          ]
        }
      ],
      "source": [
        "# You can try out different questions here.\n",
        "test_question='請問誰是 Taylor Swift？'\n",
        "\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\"},    # System prompt\n",
        "    {\"role\": \"user\", \"content\": test_question}, # User prompt\n",
        "]\n",
        "\n",
        "print(generate_response(llama3, messages))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0-ojJuE_Oln"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HGsIPud3_Oln"
      },
      "source": [
        "The TA has implemented the Agent class for you. You can use this class to create agents that can interact with the LLM model. The Agent class has the following attributes and methods:\n",
        "- Attributes:\n",
        "    - role_description: The role of the agent. For example, if you want this agent to be a history expert, you can set the role_description to \"You are a history expert. You will only answer questions based on what really happened in the past. Do not generate any answer if you don't have reliable sources.\".\n",
        "    - task_description: The task of the agent. For example, if you want this agent to answer questions only in yes/no, you can set the task_description to \"Please answer the following question in yes/no. Explanations are not needed.\"\n",
        "    - llm: Just an indicator of the LLM model used by the agent.\n",
        "- Method:\n",
        "    - inference: This method takes a message as input and returns the generated response from the LLM model. The message will first be formatted into proper input for the LLM model. (This is where you can set some global instructions like \"Please speak in a polite manner\" or \"Please provide a detailed explanation\".) The generated response will be returned as the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zjG-UwDX_Oln"
      },
      "outputs": [],
      "source": [
        "class LLMAgent():\n",
        "    def __init__(self, role_description: str, task_description: str, llm:str=\"bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\"):\n",
        "        self.role_description = role_description   # Role means who this agent should act like. e.g. the history expert, the manager......\n",
        "        self.task_description = task_description    # Task description instructs what task should this agent solve.\n",
        "        self.llm = llm  # LLM indicates which LLM backend this agent is using.\n",
        "    def inference(self, message:str) -> str:\n",
        "        if self.llm == 'bartowski/Meta-Llama-3.1-8B-Instruct-GGUF': # If using the default one.\n",
        "            # TODO: Design the system prompt and user prompt here.\n",
        "            # Format the messsages first.\n",
        "            messages = [\n",
        "                {\"role\": \"system\", \"content\": f\"{self.role_description}\"},  # Hint: you may want the agents to speak Traditional Chinese only.\n",
        "                {\"role\": \"user\", \"content\": f\"{self.task_description}\\n{message}\"}, # Hint: you may want the agents to clearly distinguish the task descriptions and the user messages. A proper seperation text rather than a simple line break is recommended.\n",
        "            ]\n",
        "            return generate_response(llama3, messages)\n",
        "        else:\n",
        "            # TODO: If you want to use LLMs other than the given one, please implement the inference part on your own.\n",
        "            return \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-ueJrgP_Oln"
      },
      "source": [
        "TODO: Design the role description and task description for each agent."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "DzPzmNnj_Oln"
      },
      "outputs": [],
      "source": [
        "# TODO: Design the role and task description for each agent.\n",
        "\n",
        "# This agent may help you filter out the irrelevant parts in question descriptions.\n",
        "question_extraction_agent = LLMAgent(\n",
        "    role_description=\"\",\n",
        "    task_description=\"\",\n",
        ")\n",
        "\n",
        "# This agent may help you extract the keywords in a question so that the search tool can find more accurate results.\n",
        "keyword_extraction_agent = LLMAgent(\n",
        "    role_description=\"\",\n",
        "    task_description=\"\",\n",
        ")\n",
        "\n",
        "# This agent is the core component that answers the question.\n",
        "qa_agent = LLMAgent(\n",
        "    role_description=\"你是 LLaMA-3.1-8B，是用來回答問題的 AI。使用中文時只會使用繁體中文來回問題。\",\n",
        "    task_description=\"請回答以下問題：\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9eoywr7_Oln"
      },
      "source": [
        "## RAG pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HDOjNYJ_Oln"
      },
      "source": [
        "TODO: Implement the RAG pipeline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRGNa-1i_Oln"
      },
      "source": [
        "Please refer to the homework description slides for hints.\n",
        "\n",
        "Also, there might be more heuristics (e.g. classifying the questions based on their lengths, determining if the question need a search or not, reconfirm the answer before returning it to the user......) that are not shown in the flow charts. You can use your creativity to come up with a better solution!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMaIsKAZ_Olo"
      },
      "source": [
        "- Naive approach (simple baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mppO-oOO_Olo"
      },
      "source": [
        "- Naive RAG approach (medium baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/naive_rag.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYxbciLO_Olo"
      },
      "source": [
        "- RAG with agents (strong baseline)\n",
        "\n",
        "    ![](https://www.csie.ntu.edu.tw/~ulin/rag_agent.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ztJkA7R7_Olo"
      },
      "outputs": [],
      "source": [
        "async def pipeline(question: str) -> str:\n",
        "    # TODO: Implement your pipeline.\n",
        "    # Currently, it only feeds the question directly to the LLM.\n",
        "    # You may want to get the final results through multiple inferences.\n",
        "    # Just a quick reminder, make sure your input length is within the limit of the model context window (16384 tokens), you may want to truncate some excessive texts.\n",
        "    return qa_agent.inference(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Answer the questions using your pipeline!"
      ],
      "metadata": {
        "id": "P_kI_9EGB0S9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since Colab has usage limit, you might encounter the disconnections. The following code will save your answer for each question. If you have mounted your Google Drive as instructed, you can just rerun the whole notebook to continue your process."
      ],
      "metadata": {
        "id": "PN17sSZ8DUg7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "# Fill in your student ID first.\n",
        "STUDENT_ID = \"123456\"\n",
        "\n",
        "STUDENT_ID = STUDENT_ID.lower()\n",
        "with open('./public.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    questions = [l.strip().split(',')[0] for l in questions]\n",
        "    for id, question in enumerate(questions, 1):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'w') as output_f:\n",
        "            print(answer, file=output_f)\n",
        "\n",
        "with open('./private.txt', 'r') as input_f:\n",
        "    questions = input_f.readlines()\n",
        "    for id, question in enumerate(questions, 31):\n",
        "        if Path(f\"./{STUDENT_ID}_{id}.txt\").exists():\n",
        "            continue\n",
        "        answer = await pipeline(question)\n",
        "        answer = answer.replace('\\n',' ')\n",
        "        print(id, answer)\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'a') as output_f:\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "plUDRTi_B39S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80167b55-c0a6-4ced-bdf7-191daa0c220d"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 \"虎山雄風飛揚 \" 是香港中文大學的校歌。\n",
            "2 對不起，我無法提供關於2025年初NCC規定的具體數字。\n",
            "3 第一代 iPhone 是由史蒂夫·喬布斯（Steve Jobs）發表的。\n",
            "4 根據台灣大學的進階英文免修申請規定，托福網路測驗 TOEFL iBT 的成績需達到 80 分以上才能符合資格。\n",
            "5 在橄欖球聯盟（Rugby Union）中，觸地試踢得 5 分。\n",
            "6 根據歷史記載，卑南族的祖先發源自ruvuwa'an，這個地點被認為是位於現今臺灣省臺東縣太麻里鄉附近。\n",
            "7 根據我的知識，熊仔的碩士指導教授是黃韻玲。\n",
            "8 答案是 該問題的解答為：詹姆斯·克拉ーク・マ克斯韋爾。\n",
            "9 距離國立臺灣史前文化博物館最近的臺鐵車站為「台東」火车駅。\n",
            "10 答案是：50\n",
            "11 對不起，我無法提供關於未來的 NBA 賽事結果或交易資訊。\n",
            "12 對不起，我無法提供關於未來事件的確切信息。2024年美國總統大選尚待進行，各候选人競爭仍在繼續中。如果您需要了解目前的情況或是過去的大事記錄，可以試著問我其他問題！\n",
            "13 根據我的知識，Llama-3.1 系列模型中參數量最小的為 LLaMA 7B（約有七十億個参数），但我無法確認是否能夠找到與你的問題相關聯之資料。\n",
            "14 根據國立臺灣大學學則，停修有更嚴格的限制。依照規定，在沒有師生雙方同意的情況下，每個班級每年最多只能申請兩門課程停止上课（即「退選」），但如果是因為個人原因需要暫時休息或轉學等情形，則可以向系辦提出停修申请。\n",
            "15 對不起，我無法找到任何有關DeepSeek公司的相關資訊。\n",
            "16 對不起，我無法提供最新的資訊或未來事件。2024年NBA總冠軍隊伍尚待確認，請關注官方新聞和賽事結果以獲得最準確且更新至今的情況報告\n",
            "17 這類化合物稱為烯（Alkene）。\n",
            "18 答案是阿倫·圖靈（Alan Turing）。他是一位英國數學家、計算機科學院院士和密碼破譯者，被認為是在現代電腦理論的奠基人。他的著作《運算器與智能》中提出「图灵机」的概念，對於發展出後來的大型電子计算机会产生了深远影响。他還在第二次世界大戰期間參加過密碼破譯工作，並為英國政府做出了重要貢獻。\n",
            "19 根據我的知識，臺灣玄天上帝信仰的進香中心位於新北市瑞芳區。\n",
            "20 Windows 作業系統是微軟公司（Microsoft）的產品。\n",
            "21 官將首的起源是由於臺灣南部一間名為「天后宮」的廟宇。根據傳說，該寺院在清代時期，由當地信徒創立了這種特殊陣頭，以供奉主神之駕前侍衛，並負責保護和押煞的職能。  官將首與八家将不同，它們是臺灣所獨有的文化象徵之一，體現出本土化傳統藝術。\n",
            "22 《咒》的邪神名為 \"阿修羅\"。\n",
            "23 這個問題的答案是：五月天。\n",
            "24 對不起，我無法提供 2025 年卑南族聯合年聚的具體資訊，因為這類型的情況可能會根據當地部落和組織者的安排而有所變動。然而，通常來說，這些活動是由各個原住民族委員会或相關機構負責籌辦。  如果你想知道 2025 年卑南族聯合年聚的具體資訊，我建議您可以查詢以下幾種途徃：  1. 卫生部保健署：他們可能會提供原住民族活動和節慶相關信息。 2.Taiwan Indigenous Peoples' Festival（臺灣土著人民文化藝術祭）：這是一個關注台灣各族群的音樂、舞蹈及戲劇表演，包括卑南人的傳統儀式等。\n",
            "25 最新的輝達顯卡是出到「GeForce RTX 40系列」。這個序列包括RTXT4090、Rtx40810等多款高端显 卡。\n",
            "26 大S（原名陳嘉樺）是在2013年6月去世的，她在美國洛杉磯因為腦部出血而不幸離開了我們。\n",
            "27 萬有引力的發現通常被歸功於英國科學家艾薩克·牛頓。他在1687年出版的著作《哲理自然法則》中，提出了三大運動定律和万有的吸力原論（Universal Gravitation），描述了物體之間彼此引力的作用。這個概念對於理解天文學、物理学等領域有重大影響，是牛頓的主要成就之一。  然而，也有一些古代哲人，如亞里斯多德和阿基米得，已經提到了類似萬有的吸力原理，但沒有發展出完整且精確的地球引力的描述。\n",
            "28 台鵠開示計畫「TAIHUCAIS」的英文全名為 \"Taiwan Indigenous Cultural Assets Information System\"。\n",
            "29 「I'll be back」是出自1984年電影《終極追殺令》（The Terminator）的經典台詞，由阿諾·施瓦辛格飾演的角色泰倫納斯（T-800）說出的。\n",
            "30 水的化學式是 H₂O。\n",
            "31 對不起，我無法提供李宏毅教授在《機器學習》2023年春季班中第15個作業的名稱。\n",
            "32 根據我的知識，目前臺灣公立的獨립學院僅剩一間是國防管理学院（簡稱：德明科大）。\n",
            "33 BitTorrent 協議使用的機制是叫做 \"Tracker\" 和 \"_piece_選擇算法（Piece Selection Algorithm）或稱為 _bitfield_\"。  當一個新的節點加入網路時，該協定會要求 Tracker 服務器提供一份包含所有種子的資訊清單。這些信息包括每個种子拥有的块数和哪个block是已下载的、未完成等状态。这使得新节点可以知道其他节点拥有什么样的数据。  接下来的步骤，節點會使用 _bitfield_ 機制來選擇從何處取得資料。它們会將自己的 bit field 和所有种子的.bitfiled進行比對，以找出哪些块是自己还没有的，并且其他节点已经有了。这樣就能確保新加入網路節點可以从已經拥有所需数据片段（chunk）的種子中隨機地下载資料。  這個 _bitfield_ 機制使得 BitTorrent 協議能够有效率和公平 地分配资源，避免某些节点过载或其他问题。\n",
            "34 這個描述聽起來很像是一部名為\"Crash Course: University of California, Berkeley to Stanford on a Motorcycle (GoPro)\"的影片。\n",
            "35 根據研究人員的觀察報告和實驗結果，戈芬氏鳳頭鸚鵡最受它們偏好的乳酪口味是什麼呢？答案是在這個文中並沒有明確提及。\n",
            "36 很抱歉，我無法找到相關的資訊，關於2024年桃園Xpark水族館國王企鵝「嘟胖」和 「烏龍茶」的新生仔。\n",
            "37 根據國立臺灣大學的資訊，物理治療學系目前正常修業年限為四年的碩士班和三年半或五個月左右（含實習）的本科生課程。\n",
            "38 《BanG Dream!》中，「呼嘿哈」或是 「呵～咯」的笑聲習慣，是由Riko Kawamura所扮演的角色。\n",
            "39 你提到的「甲斐之虎」其實是日本戰國時代的一位著名武將——高良政永的別稱，但最為人所知的是另一人的。正確答案應該是在德川家康統一天下後，成為幕府第一代征夷大元勳、甲斐守護職和信濃郡主等領地持有者，並且被尊封「武藏」公的——高良政永並非最為人所知。\n",
            "40 根據王肥貓同學的標準，他想要選擇網路上最多好評的一門課。因此，我們需要找出這三個候補中的哪一堂是台大生中受歡迎度最高、口碑最佳。  雖然我沒有直接存取到最新版資料，但根據過往的資訊，「數位素養導航」是一道非常受到好評和喜愛的一門課。這個選修科目幫助學生了解如何在現代社會中有效地使用科技工具，並培育他們對於信息檢視、分析與創造力的能力。  因此，我們可以推測王肥貓同事最有可能去上「數位素養導航」這堂課。\n",
            "41 對不起，我無法提供2024年的第42回《極限體能王SASUKE》首播日期的資訊，因為我沒有最新的情況。\n",
            "42 根據歷史記載，出身於利嘉部落後來成為初鹿（或稱為阿美族的「卡馬蘭」）頭目的漢人，是名叫張永芳。\n",
            "43 《BanG Dream! Ave Mujica》的片頭曲是「」。\n",
            "44 Linux作業系統最早於1991年首次發布。由林納斯·托瓦茲（Linus Torvalds）在芬蘭赫爾辛基大學開源發展的 Linux核心，最初是為了取代MINIX而設計的一個Unix-like操作系统，並且迅速演變成一個完整、可用的作業系統。在1991年9月25日，他發布了一份名叫\"Linux 0.01”的原始碼。\n",
            "45 根據你的描述，Likavung 部落的中文名稱應該是「利卡武崗部族」或簡稱為 \" 利加邦\"\n",
            "46 紅茶是一種半發酵或部分生抽的黑tea。\n",
            "47 在《遊戲王》卡牌中，以「真紅眼黑龍」與 「黒魔導士」的融合素材，會產生出名為 \"Black Luster Soldier of Destruction\" 的怪獸。\n",
            "48 豐田萌繪在《BanG Dream!》企劃中，擔任Poppin'Party的聲優。\n",
            "49 Rugby Union 中，9 號球員的正式名稱為「掃邊後衛」（Scrum-half）。\n",
            "50 被降格成矮行星的主要原因是該天體沒有清晰地圍繞太陽公轉，且其軌道與其他大型物質相比過於混亂。根據這些標準，最著名的一個例子就是冥王 星（Pluto）。\n",
            "51 根據我的知識，臺灣最早成立的野生動物救傷單位是位於宜蘭縣。\n",
            "52 根據我的知識，特生中心在2023年改名後，被重新命為「集結動物園」。\n",
            "53 根據我的知識庫，Developing Instruction-FollowING Speech Language Model Without speech instruction-tuning data論文中提出的模型是名為\"Instruction-Tuned (IT) 模型的前身，即 \"Prompt-Modulated Pre-training and Fine TuninG（PPFG）\"\"\n",
            "54 太陽系中體積最大的行星是土壤外的木質天王。\n",
            "55 根據語言分類學的研究，臺灣原住民族中的阿美族（Amis）和卑南人等部落使用的是屬於北台灣亞東群島支系下的雅馬莫-大武壠方塊的一種特殊變體。這些與其他台湾本岛語言相比，在分類學上一般不被視為同一族的原住民族是卑南人（Paiwan）。\n",
            "56 很抱歉，我無法確認這句話的老師是誰。然而，這個故事可能源自於台灣大學資訊工程學系的一位教授，許智傑（也稱為「程式設計教父」）。他曾經在台大上課，並且以其幽默和風趣而聞名。  但是，我無法確認這句話是否真實地出現在他的口中，或是只是個都市傳說。\n",
            "57 「embiyax namu kana」是阿美族的打招呼用語。\n",
            "58 根據我的知識，「鄒與布農, 永久美麗」這句話是指位於台東縣的達仁鄉的一個部落——大武山。該地區因為地理位置特殊，被認爲是一處天然之門，因此在歷史上曾經有許多不同族群的人民聚居於此，包括鄒、布農等民族。  這句話可能是用來描述達仁鄉的大 武 山部落的美麗景色和豐富文化，也可以視為對該地區人民友好相處與融合之象徵。\n",
            "59 很抱歉，我無法找到相關的資訊。\n",
            "60 根據卑南族的傳說，姊弟 Tuku 及 Sihasihau 分別創建了 Amis 和 Paiwan 兩個部落。\n",
            "61 《終極一班》中的「KO榜」是高中生戰力排行的名單，該劇中提到 KO 排在第 1 位的是 \"阿飛\"。\n",
            "62 Linux kernel 中的 Completely Fair Scheduler (CFS) 使用紅黑樹（Red-Black Tree）來儲存排程相關資訊。這種資料結構能夠有效地維護和查找各個進程序所在位置，從而實現公平且高效率的情況下進行任務的調度。  CFS 會根據每一個作業系統線路（task）的nice值、執行時間等因素來計算其優先順序，並將這些資訊儲存於紅黑樹中。當需要選擇哪個進程要被運算時，Linux kernel 就可以通過查找和比較各種資料結構中的元素，以確保公平性並達到最佳的效率。  因此，可以說CFS使用了Red-Black Tree來儲存排行相關資訊。\n",
            "63 諾曼第登陸（Normandy Landings）的作戰代號為「奧運會」（Operation Overlord）。\n",
            "64 《Cytus II》遊戲中「Body Talk」是由TOMOSUKE的角色歌曲。\n",
            "65 李琳山教授開設的信號與系統課程，期末考前後的一次演講被稱為「最後一堂上學」。\n",
            "66 根據最新的資訊，NVIDIA RTX 5090 顯卡確實具有顯著提升之記憶體量。它擁有高達48GB GDDR6x VRAM，這使得使用者可以部署更大的LLM（大型語言模型），並提供出色的計算效能和內存容納能力。  這樣的設計讓RTX 5090顯卡成為了一款頂級選項，適合那些需要高性能運算、龐大人工智慧應用或是深度學習工作的人。\n",
            "67 對不起，我無法提供2024年世界棒球12強賽的冠軍隊伍資訊，因為我沒有存取到最新或未來事件相關資料。\n",
            "68 中國四大奇書是指《西遊記》、《水滸傳》，以及兩本古典長篇小說： 《三國演義》（又稱為「隆中對」）和『金瓶梅』。\n",
            "69 子時是中國傳統的時間制，相當於西方世界中的凌晨。根據24小 時 制，如果用 12 小时 表示，那么 子 时 就是在午夜到早上5點之間。  因此，用23:00至05：59表示即可\n",
            "70 在作業系統中，避免要錯過時限來完成任務的排程演算法稱為「即期式」或是\"Real-time Scheduling\"(RTS)。這類型的手段會根據每個程序或者進驅動器所需執行完畢時間以及優先順序，確保系統能夠在指定時限內完成任務。  然而，在一般的作業環境中，更常見的是使用「非即期式」或是\"Non-Real-time Scheduling\"(NRTS) 的排程演算法，如FCFS（First-Come First-Served）、SJF(Shortest Job Fist)、SRTN(Sporadic Server Task Model)，等。這類型的手段不一定能夠確保任務在指定時限內完成，但可以根據系統的狀態和優先順序，進行動态調整以提高效率。  值得注意的是，即期式排程演算法通常需要更高級別的心臟跳躍（interrupt）機制，以便能夠及早地識别任務完成時間逾時，並對系統做出適當的反應。\n",
            "71 在《刀劍神域》中，「C8763」是由Sachi的技能代碼。\n",
            "72 《斯卡羅》是一部以加拿大西北地區為背景的歷史小說，描述了克里族人與白人的衝突。劇中之地名「柴城」位於現今英屬哥倫比亞省東方內陸區域，也就是現在的大熊湖國家公園附近。  但如果你是指《斯卡羅》(1990年)的電視連續剧，那么它描述的是一部以加拿大西北地區為背景的小說改編劇，柴城（Fort Hargrave）則位於現今阿爾伯塔省南方。\n",
            "73 根據Google Colab的最新資訊，若要使用A100高級GPU，您需要訂閱「Colabs Pro+」。\n",
            "74 李宏毅老師開設的機器學習課程是屬於台大資訊工程學院（College of Electrical Engineering and Computer Science）的電信傳播研究所或計算科研中心。\n",
            "75 根據國立臺灣大學的規定，學生每年修滿一定數量的心理健康課程和體育活動就不需要簽減免申請書。一般而言，這些要求大約在 2-3 學分左右。  另外，一般來說，大三級生的總計必選、限額及其他非可抵低學力修業年數之科目至少應達到一定的門檻，通常是每年的課程需求。一般而言，這個要求大約在 30-40 學分左右。  因此，如果雪江同事想要避免簽減少申請書，他需要確保自己已經修滿了這些基本學科和總計必選、限額及其他非可抵低学力年數之課程的需求。至少要達到 30-40 學分左右。  但是，具體要求可能會因為雪江同事所屬系級以及當年的教務處規定而有不同，因此建議他直接查詢相關資訊或與學生事務組聯繫以確保正確認識。\n",
            "76 Neuro-sama 的最初 Live2D 模型是使用 VTube Studio 預設角色 \"VTuber\"。\n",
            "77 在「從零開始的異世界生活 第三季」動畫中，劫持愛蜜莉雅並想取其為妻的人是雷德（Ryuuji）和尤里烏斯·福爾卡特。\n",
            "78 《海綿寶宝》的主角在第五季的劇集中，擊敗刺破泡沫紅眼幇是在布魯克林市。\n",
            "79 玉米是一種雙子葉植物。\n",
            "80 中華民國陸軍的前六字為：\"忠義勇猛，奮鬥到底。\"\"\n",
            "81 根據台大電資學院的規定，計算機科學與信息工程系（CSIE）是其中一個例外。這個部門認為，在專業領域中，這些自然 科技知識並不一定需要，因此允許選修一至兩項。  然而，我們必須注意到，由於教育政策的變化和學院內各科學位課程設計不同，可能會有所改動。因此，最好的方法是直接查詢台大電資学院或相關系部網站，或聯繫教務處以確認最新的情況。  另外，也可以參考以下幾個選項：  1.  台北大學的計算機科學與信息工程研究所（CSIE）同樣有這種規定。 2\\. 電信傳播學院：電訊系、通識資工所等相關課程設計中，物理和化驗實際上並不需要。  但請注意，這些情況可能會因為政策變更而改動，因此最好是直接查詢台大或各個學科的網站以確認最新的情形。\n",
            "82 憂傷湖（Lacus Doloris）、死lake （不確定是不是 Lacus Morti s） 、忘海  ( 不一定正解) 和恐怖 lake(同上)、愛灣(Sinus Amoris)，以上四個地形都位於月球的背面。\n",
            "83 《C♯小調第14號鋼琴奏鳴曲》又被稱為「月光鈴」，這是因爲它的第二樂章中有著一段美麗而優雅的情節音樂，讓人聯想到夜晚下雨時敘述的一首古老的小詩。\n",
            "84 阿米斯音樂節（Coachella Valley Music and Arts Festival）是一個國際知名的音乐节，於1999年由保羅·費爾德許和亞倫‧萊文所創辦。\n",
            "85 在 \"Poppy Playtime - Chapter 4\" 遊戲中，黏土人被稱為「Huggy Wugs」。\n",
            "86 根據你的問題，賓茂村其實屬於達仁鄉。\n",
            "87 米開朗基羅的《大衛》雕像最初是在佛罗伦萨的一個花崗岩石坑中被創作出來。這座著名的大理壇作品於1501年至1520年代間完成，並在1512-1564 年期間展覽，之後移到米蘭的聖母院（Santa Maria delle Grazie），並一直留存到1796年的拿破崙時期。  然而，在1815 到 1873 之间，這座雕像被運回佛羅倫斯，並在花園劇場內展示。最後，它於1882 年返回米蘭的聖母院，直至現在仍然展覽著。在2016年，由于圣玛利亚大教堂受损严重，因此《达维德》雕像被转移到附近的一个临时馆舍中。  這座作品是義大利文藝復興時期最具代表性的塑造之一，展示了米開朗基羅對人體解剖學和比例的深入理解，以及他在創作上的技巧。\n",
            "88 根據中華民國軍事史，除了蔣介石外，一位曾短暫晉升特級上將的将領是嚴家淦。\n",
            "89 2012年英雄聯盟世界大賽的總冠軍是韓國戰隊「SK Telecom T1」。\n",
            "90 在日本麻將中，非莊家一開始的手牌通常有14張。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the results into one file.\n",
        "with open(f'./{STUDENT_ID}.txt', 'w') as output_f:\n",
        "    for id in range(1,91):\n",
        "        with open(f'./{STUDENT_ID}_{id}.txt', 'r') as input_f:\n",
        "            answer = input_f.readline().strip()\n",
        "            print(answer, file=output_f)"
      ],
      "metadata": {
        "id": "GmLO9PlmEBPn"
      },
      "execution_count": 30,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}